{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e90a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(574, 1882)\n",
      "(115, 454)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #numpy and pandas: These libraries are essential for handling and processing data. \n",
    "import pandas as pd #numpy is used for numerical operations, and pandas is used for handling data in a tabular format.\n",
    "from sklearn.model_selection import train_test_split #train_test_split: To divide our dataset into a training set (used to train the model) and a test set (used to evaluate the model).\n",
    "from sklearn.svm import SVC #SVC and classification_report: To classify the data and evaluate the classification performance.\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Load the new dataset\t\n",
    "#We need the data to work with. \n",
    "#Thyloid.csv contains the gene expression measurements and the stage information of patients.\n",
    "data = pd.read_csv('Thyloid.csv')  #Reads the dataset from a CSV file into a pandas DataFrame.\n",
    "print(data.shape) #Prints the shape of the dataset (number of rows and columns).\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "#We split the data so we can train the model on one part (training set) \n",
    "#and test how well it performs on unseen data (testing set). \n",
    "#This helps us evaluate the model’s performance realistically.\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42) #Splits the dataset into training (80%) and testing (20%) sets.\n",
    "\n",
    "# storing the labels separately\n",
    "#The last column contains the labels (stage information), \n",
    "#which we need to separate from the features (gene expressions) for training and evaluation purposes.\n",
    "train_label = train_data.iloc[:, -1]\n",
    "test_label = test_data.iloc[:, -1]\n",
    "\n",
    "# removing the last column from the data itself\n",
    "#Removes the label column from the features.\n",
    "train_X = train_data.iloc[:, :-1]\n",
    "test_X = test_data.iloc[:, :-1]\n",
    "\n",
    "# converting to numpy\n",
    "#Most machine learning algorithms work efficiently with numpy arrays, so we convert the data for further processing.\n",
    "train = train_X.to_numpy()\n",
    "test = test_X.to_numpy()\n",
    "\n",
    "\n",
    "#Kernel functions: These functions transform the data into a higher-dimensional space to make it easier to find patterns. \n",
    "#Different kernels are used based on the data characteristics:\n",
    "\n",
    "#RBF Kernel: Useful for non-linear relationships.\n",
    "# Computes the RBF kernel between two data points x and y.\n",
    "def rbf_kernel(x, y, gamma=1.0):\n",
    "    \"\"\" Radial Basis Function (RBF) Kernel \"\"\"\n",
    "    return np.exp(-gamma * np.linalg.norm(x - y) ** 2)\n",
    "\n",
    "#Polynomial Kernel: Captures polynomial relationships between data points.\n",
    "#Computes the Polynomial kernel between two data points x and y.\n",
    "def poly_kernel(x, y, degree=3):\n",
    "    \"\"\" Polynomial Kernel \"\"\"\n",
    "    return np.dot(x, y) ** degree\n",
    "\n",
    "#Linear Kernel: Useful for linear relationships.\n",
    "# Computes the Linear kernel between two data points x and y.\n",
    "def linear_kernel(x, y):\n",
    "    \"\"\" Linear Kernel \"\"\"\n",
    "    return np.dot(x, y)\n",
    "\n",
    "\n",
    "#Kernel matrix: This matrix contains the similarity measures between all pairs of data points in the dataset using the chosen kernel. \n",
    "#It’s the first step in Kernel PCA, which transforms the data into a space where it’s easier to find patterns.\n",
    "#Computes the kernel matrix for the given data using the specified kernel type (rbf, poly, or linear) and kernel parameter (e.g., gamma for RBF, degree for Polynomial).\n",
    "def compute_kernel_matrix(data, kernel_type, kernel_param):\n",
    "    n_samples = data.shape[0]\n",
    "    kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            if kernel_type == 'rbf':\n",
    "                kernel_matrix[i, j] = rbf_kernel(data[i], data[j], kernel_param)\n",
    "            elif kernel_type == 'poly':\n",
    "                kernel_matrix[i, j] = poly_kernel(data[i], data[j], kernel_param)\n",
    "            elif kernel_type == 'linear':\n",
    "                kernel_matrix[i, j] = linear_kernel(data[i], data[j])\n",
    "\n",
    "    return kernel_matrix, n_samples\n",
    "\n",
    "\n",
    "#Implements Kernel PCA from scratch.\n",
    "#Kernel PCA: This is an advanced form of PCA that uses kernel functions to handle non-linear data.\n",
    "def my_kpca(data, alpha, kernel_type='rbf', kernel_param=1.0):\n",
    "    kernel_matrix, n_samples = compute_kernel_matrix(data, kernel_type, kernel_param)   #compute_kernel_matrix: Computes the kernel matrix.\n",
    "\n",
    "    # Ensure the kernel matrix is symmetric\n",
    "    #Centering the kernel matrix: Ensures the mean of the data is zero in the transformed space.\n",
    "    kernel_matrix = (kernel_matrix + kernel_matrix.T) / 2 \n",
    "    \n",
    "    one_n = np.ones((n_samples, n_samples)) / n_samples #Creates a matrix of ones.\n",
    "    \n",
    "    mean2 = one_n.dot(kernel_matrix).dot(one_n)  #Computes the double-centered mean.\n",
    "    centered_kernel_matrix = kernel_matrix - one_n.dot(kernel_matrix) - kernel_matrix.dot(one_n) + mean2 #Centers the kernel matrix.\n",
    "\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(centered_kernel_matrix) #Computes the eigenvalues and eigenvectors of the centered kernel matrix.\n",
    "\n",
    "    # Ensure eigenvalues and eigenvectors are real\n",
    "    #Eigenvalues and eigenvectors: \n",
    "    #These help in identifying the principal components that capture the most variance in the data.\n",
    "    eigen_values = np.real(eigen_values)\n",
    "    eigen_vectors = np.real(eigen_vectors)\n",
    "\n",
    "    idx = np.argsort(eigen_values)[::-1] #Sorts the eigenvalues in descending order.\n",
    "    eigen_values = eigen_values[idx]\n",
    "    eigen_vectors = eigen_vectors[:, idx]\n",
    "    \n",
    "    #Variance ratio: \n",
    "    #Helps in deciding how many principal components to retain to capture a significant amount of variance \n",
    "    #(alpha is the threshold for this).\n",
    "    #var_ratio and cumulative_var_ratio: Calculates the explained variance and cumulative explained variance.\n",
    "    var_ratio = eigen_values / np.sum(eigen_values)\n",
    "    cumulative_var_ratio = np.cumsum(var_ratio)\n",
    "    k = np.argmax(cumulative_var_ratio >= alpha) + 1 #Determines the number of components to retain to achieve the desired explained variance (alpha).\n",
    "    reduced_eigen_vectors = eigen_vectors[:, :k]\n",
    "\n",
    "    projected_data = np.dot(centered_kernel_matrix, reduced_eigen_vectors)\n",
    "\n",
    "    return projected_data, reduced_eigen_vectors, mean2, k\n",
    "\n",
    "#Apply KPCA to new data: \n",
    "#After training the KPCA model, we need to project new data (test data) into the same transformed space. \n",
    "#This ensures consistency in how both training and test data are transformed.\n",
    "\n",
    "def kPCA_NewData(Y, X, eigVector, type='gaussian', para=1):  #Applies the KPCA transformation to new data using the trained eigenvectors.\n",
    "    combined_data = np.vstack([Y, X])  #Combines the new data (Y) and the training data (X).\n",
    "    K_combined, _ = compute_kernel_matrix(combined_data, type, para)   #K_combined: Computes the kernel matrix for the combined data.\n",
    "    K = K_combined[:Y.shape[0], Y.shape[0]:] #K: Extracts the relevant part of the kernel matrix.\n",
    "\n",
    "    Z = np.dot(K, eigVector)  #Z: Projects the new data into the reduced-dimensional space using the trained eigenvectors.\n",
    "\n",
    "    return Z\n",
    "\n",
    "# Apply KPCA to training data\n",
    "#Reduces the dimensionality of the training data to a lower-dimensional \n",
    "#space where the most important features (principal components) are retained.\n",
    "projected_train_kpca, reduced_eigen_vectors, K_centered_train, k = my_kpca(train, alpha=0.99, kernel_type='rbf', kernel_param=0.5   #Applies KPCA to the training data with 99% explained variance using an RBF kernel with gamma=0.5.\n",
    "\n",
    "# Apply KPCA for the test data\n",
    "#Projects the test data into the same reduced-dimensional space as the training data, \n",
    "#ensuring consistency in data representation.\n",
    "test_reduced = kPCA_NewData(test, train, reduced_eigen_vectors, type='gaussian', para=0.5)\n",
    "\n",
    "#Helps verify that the transformation has been applied correctly and the dimensions are as expected.\n",
    "print(test_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbedaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
