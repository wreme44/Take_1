{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f9f3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Load the dataset to inspect its columns\n",
    "# file_path = 'Thyloid.csv'\n",
    "# data = pd.read_csv(file_path)\n",
    "# print(data.head())\n",
    "# print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00873bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c074b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_impute_data(file_path):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Handle missing values by imputing with the mean\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "    \n",
    "    return data_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b56c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c05edeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    #Normalize the data by subtracting the mean and\n",
    "    # dividing by the standard deviation\n",
    "    return (data - data.mean()) / data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2519f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance_matrix(data):\n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = np.cov(data.T)\n",
    "    return covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab7373fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvalues_eigenvectors(covariance_matrix):\n",
    "    #Compute eigenvalues and Eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    return eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09e2dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_eigenvalues_eigenvectors(eigenvalues, eigenvectors):\n",
    "    #Sort the eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    return eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cedd893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_data (data, eigenvectors, num_components):\n",
    "    #Project the data onto the selected eigenvectors\n",
    "    return np.dot(data, eigenvectors[:, :num_components])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d62322d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced data shape: (574, 1881)\n",
      "Number of components retained: 1881\n"
     ]
    }
   ],
   "source": [
    "def pca_from_scratch(file_path, variance_threshold=0.95):\n",
    "    # Load and impute the data\n",
    "    data = load_and_impute_data(file_path)\n",
    "    labels = data['Label']  \n",
    "    features = data.drop(columns=['Label'])\n",
    "    normalized_data = normalize_data(features)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = compute_covariance_matrix(normalized_data)\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = compute_eigenvalues_eigenvectors(covariance_matrix)\n",
    "\n",
    "    # Sort the eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = sort_eigenvalues_eigenvectors(eigenvalues, eigenvectors)\n",
    "\n",
    "    # Determine the number of principal components to retain the desired variance\n",
    "    total_variance = sum(eigenvalues)\n",
    "    variance_explained = 0\n",
    "    num_components = 0\n",
    "    for eigenvalue in eigenvalues:  # Correct the typo here\n",
    "        variance_explained += eigenvalue\n",
    "        num_components += 1\n",
    "        if variance_explained / total_variance >= variance_threshold:\n",
    "            break\n",
    "\n",
    "    # Project the data onto the principal components\n",
    "    reduced_data = project_data(normalized_data, eigenvectors, num_components)\n",
    "\n",
    "    return reduced_data, num_components\n",
    "\n",
    "# Usage\n",
    "file_path = 'Thyloid.csv'\n",
    "reduced_data, num_components = pca_from_scratch(file_path)\n",
    "print(f'Reduced data shape: {reduced_data.shape}')\n",
    "print(f'Number of components retained: {num_components}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922aaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70a577a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Apply PCA using scikit-learn\u001b[39;00m\n\u001b[1;32m     10\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)  \u001b[38;5;66;03m# Retain 95% of variance\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m reduced_data_sklearn \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(normalized_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReduced data shape (sklearn): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduced_data_sklearn\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExplained variance ratio (sklearn): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpca\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 462\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m    463\u001b[0m U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA does not support sparse input. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncatedSVD for a possible alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    486\u001b[0m     X, dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32], ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[1;32m    922\u001b[0m             array,\n\u001b[1;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# PCA using scikit-learn\n",
    "data = load_and_impute_data(file_path)\n",
    "labels = data['Label']\n",
    "features = data.drop(columns=['Label'])\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = normalize_data(features)\n",
    "\n",
    "# Apply PCA using scikit-learn\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "reduced_data_sklearn = pca.fit_transform(normalized_data)\n",
    "\n",
    "print(f'Reduced data shape (sklearn): {reduced_data_sklearn.shape}')\n",
    "print(f'Explained variance ratio (sklearn): {pca.explained_variance_ratio_}')\n",
    "print(f'Number of components retained (sklearn): {pca.n_components_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c949fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Usage for RBF Kernel\u001b[39;00m\n\u001b[1;32m     29\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m---> 30\u001b[0m alphas, lambdas \u001b[38;5;241m=\u001b[39m kpca(normalized_data, \u001b[38;5;28;01mlambda\u001b[39;00m X: rbf_kernel(X, gamma), num_components)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKPCA (RBF Kernel) - reduced data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malphas\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_data' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def rbf_kernel(X, gamma):\n",
    "    # Compute the RBF (Gaussian) kernel\n",
    "    sq_dists = squareform(pdist(X, 'sqeuclidean'))\n",
    "    K = np.exp(-gamma * sq_dists)\n",
    "    return K\n",
    "\n",
    "def kpca(X, kernel, n_components):\n",
    "    # Center the kernel matrix\n",
    "    N = X.shape[0]\n",
    "    K = kernel(X)\n",
    "    one_n = np.ones((N, N)) / N\n",
    "    K_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n",
    "\n",
    "    # Compute the eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = eigh(K_centered)\n",
    "    eigenvalues, eigenvectors = eigenvalues[::-1], eigenvectors[:, ::-1]\n",
    "    eigenvectors = eigenvectors / np.sqrt(eigenvalues[:n_components])\n",
    "\n",
    "    # Select the top eigenvectors\n",
    "    alphas = eigenvectors[:, :n_components]\n",
    "    lambdas = eigenvalues[:n_components]\n",
    "\n",
    "    return alphas, lambdas\n",
    "\n",
    "# Usage for RBF Kernel\n",
    "gamma = 15\n",
    "alphas, lambdas = kpca(normalized_data, lambda X: rbf_kernel(X, gamma), num_components)\n",
    "\n",
    "print(f'KPCA (RBF Kernel) - reduced data shape: {alphas.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e42accf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      6\u001b[0m coef0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m alphas_poly, lambdas_poly \u001b[38;5;241m=\u001b[39m kpca(normalized_data, \u001b[38;5;28;01mlambda\u001b[39;00m X: polynomial_kernel(X, degree, coef0), num_components)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKPCA (Polynomial Kernel) - reduced data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malphas_poly\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_data' is not defined"
     ]
    }
   ],
   "source": [
    "def polynomial_kernel(X, degree, coef0):\n",
    "    return (X @ X.T + coef0) ** degree\n",
    "\n",
    "# Usage for Polynomial Kernel\n",
    "degree = 3\n",
    "coef0 = 1\n",
    "alphas_poly, lambdas_poly = kpca(normalized_data, lambda X: polynomial_kernel(X, degree, coef0), num_components)\n",
    "\n",
    "print(f'KPCA (Polynomial Kernel) - reduced data shape: {alphas_poly.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "246ecadb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X \u001b[38;5;241m@\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Usage for Linear Kernel\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m alphas_linear, lambdas_linear \u001b[38;5;241m=\u001b[39m kpca(normalized_data, linear_kernel, num_components)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKPCA (Linear Kernel) - reduced data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malphas_linear\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_data' is not defined"
     ]
    }
   ],
   "source": [
    "def linear_kernel(X):\n",
    "    return X @ X.T\n",
    "\n",
    "# Usage for Linear Kernel\n",
    "alphas_linear, lambdas_linear = kpca(normalized_data, linear_kernel, num_components)\n",
    "\n",
    "print(f'KPCA (Linear Kernel) - reduced data shape: {alphas_linear.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0f124cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Usage for Combined Kernels\u001b[39;00m\n\u001b[1;32m      5\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m----> 6\u001b[0m combined_k \u001b[38;5;241m=\u001b[39m combined_kernel(normalized_data, \u001b[38;5;28;01mlambda\u001b[39;00m X: rbf_kernel(X, gamma), \u001b[38;5;28;01mlambda\u001b[39;00m X: polynomial_kernel(X, degree, coef0), alpha)\n\u001b[1;32m      7\u001b[0m alphas_combined, lambdas_combined \u001b[38;5;241m=\u001b[39m kpca(normalized_data, \u001b[38;5;28;01mlambda\u001b[39;00m X: combined_k, num_components)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKPCA (Combined Kernels) - reduced data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malphas_combined\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_data' is not defined"
     ]
    }
   ],
   "source": [
    "def combined_kernel(X, kernel1, kernel2, alpha=0.5):\n",
    "    return alpha * kernel1(X) + (1 - alpha) * kernel2(X)\n",
    "\n",
    "# Usage for Combined Kernels\n",
    "alpha = 0.5\n",
    "combined_k = combined_kernel(normalized_data, lambda X: rbf_kernel(X, gamma), lambda X: polynomial_kernel(X, degree, coef0), alpha)\n",
    "alphas_combined, lambdas_combined = kpca(normalized_data, lambda X: combined_k, num_components)\n",
    "\n",
    "print(f'KPCA (Combined Kernels) - reduced data shape: {alphas_combined.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "614bd627",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top_indices\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate covariance matrix\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cov_matrix \u001b[38;5;241m=\u001b[39m compute_covariance_matrix(normalized_data)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Identify top 10 features with highest covariance\u001b[39;00m\n\u001b[1;32m     10\u001b[0m top_features_indices \u001b[38;5;241m=\u001b[39m top_features_by_covariance(cov_matrix)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_data' is not defined"
     ]
    }
   ],
   "source": [
    "def top_features_by_covariance(cov_matrix, top_n=10):\n",
    "    covariances = np.diag(cov_matrix)\n",
    "    top_indices = np.argsort(covariances)[-top_n:]\n",
    "    return top_indices\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = compute_covariance_matrix(normalized_data)\n",
    "\n",
    "# Identify top 10 features with highest covariance\n",
    "top_features_indices = top_features_by_covariance(cov_matrix)\n",
    "top_features = features.columns[top_features_indices]\n",
    "\n",
    "print(f'Top 10 features with highest covariance: {top_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a1a0a90",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1667927020.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    accuracy_kpca_rbf = calculate_accuracy(test_labels,\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(normalized_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform the test data using PCA and KPCA models\n",
    "test_data_pca = project_data(test_data, eigenvectors, num_components)\n",
    "test_data_kpca_rbf = kpca(test_data, lambda X: rbf_kernel(X, gamma), num_components)[0]\n",
    "\n",
    "# Perform classification using the minimum distance classifier\n",
    "predictions_pca = myclassifier(train_data, train_labels, test_data_pca)\n",
    "predictions_kpca_rbf = myclassifier(train_data, train_labels, test_data_kpca_rbf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_pca = calculate_accuracy(test_labels, predictions_pca)\n",
    "accuracy_kpca_rbf = calculate_accuracy(test_labels,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d74e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
